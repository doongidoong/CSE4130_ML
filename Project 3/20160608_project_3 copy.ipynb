{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.path.expanduser = lambda path: './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 60\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "import random, os\n",
    "os.environ['PYTHONHASHSEED']='0'\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,    inter_op_parallelism_threads=1,\n",
    "                                                               allow_soft_placement=True, device_count = {'CPU': 1}))\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "kernel_initializer='glorot_uniform'\n",
    "activation_function = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DROP OUT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MODEL1 : Drop out= 0.2 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL1 : Drop out= 0.2 학습 \n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "375/375 [==============================] - 7s 16ms/step - loss: 1.1964 - accuracy: 0.6194 - val_loss: 0.7336 - val_accuracy: 0.7558\n",
      "Epoch 2/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.7406 - accuracy: 0.7520 - val_loss: 0.6155 - val_accuracy: 0.7909\n",
      "Epoch 3/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.6476 - accuracy: 0.7793 - val_loss: 0.5576 - val_accuracy: 0.8116\n",
      "Epoch 4/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5926 - accuracy: 0.7969 - val_loss: 0.5255 - val_accuracy: 0.8216\n",
      "Epoch 5/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5588 - accuracy: 0.8096 - val_loss: 0.5021 - val_accuracy: 0.8297\n",
      "Epoch 6/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5347 - accuracy: 0.8160 - val_loss: 0.4841 - val_accuracy: 0.8358\n",
      "Epoch 7/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5134 - accuracy: 0.8234 - val_loss: 0.4705 - val_accuracy: 0.8392\n",
      "Epoch 8/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4967 - accuracy: 0.8299 - val_loss: 0.4583 - val_accuracy: 0.8436\n",
      "Epoch 9/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4863 - accuracy: 0.8313 - val_loss: 0.4557 - val_accuracy: 0.8409\n",
      "Epoch 10/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4728 - accuracy: 0.8353 - val_loss: 0.4435 - val_accuracy: 0.8467\n",
      "Epoch 11/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4652 - accuracy: 0.8387 - val_loss: 0.4387 - val_accuracy: 0.8476\n",
      "Epoch 12/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4571 - accuracy: 0.8416 - val_loss: 0.4285 - val_accuracy: 0.8503\n",
      "Epoch 13/60\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.4450 - accuracy: 0.8445 - val_loss: 0.4262 - val_accuracy: 0.8508\n",
      "Epoch 14/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.4399 - accuracy: 0.8474 - val_loss: 0.4171 - val_accuracy: 0.8553\n",
      "Epoch 15/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.4329 - accuracy: 0.8498 - val_loss: 0.4088 - val_accuracy: 0.8576\n",
      "Epoch 16/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4258 - accuracy: 0.8499 - val_loss: 0.4091 - val_accuracy: 0.8582\n",
      "Epoch 17/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4202 - accuracy: 0.8533 - val_loss: 0.4020 - val_accuracy: 0.8595\n",
      "Epoch 18/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4153 - accuracy: 0.8535 - val_loss: 0.3994 - val_accuracy: 0.8622\n",
      "Epoch 19/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4109 - accuracy: 0.8562 - val_loss: 0.3921 - val_accuracy: 0.8627\n",
      "Epoch 20/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4043 - accuracy: 0.8577 - val_loss: 0.3897 - val_accuracy: 0.8638\n",
      "Epoch 21/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3971 - accuracy: 0.8614 - val_loss: 0.3866 - val_accuracy: 0.8643\n",
      "Epoch 22/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3956 - accuracy: 0.8608 - val_loss: 0.3874 - val_accuracy: 0.8635\n",
      "Epoch 23/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3908 - accuracy: 0.8615 - val_loss: 0.3814 - val_accuracy: 0.8653\n",
      "Epoch 24/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3880 - accuracy: 0.8626 - val_loss: 0.3765 - val_accuracy: 0.8689\n",
      "Epoch 25/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3829 - accuracy: 0.8657 - val_loss: 0.3739 - val_accuracy: 0.8687\n",
      "Epoch 26/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3781 - accuracy: 0.8659 - val_loss: 0.3706 - val_accuracy: 0.8702\n",
      "Epoch 27/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3759 - accuracy: 0.8676 - val_loss: 0.3696 - val_accuracy: 0.8696\n",
      "Epoch 28/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3716 - accuracy: 0.8669 - val_loss: 0.3676 - val_accuracy: 0.8703\n",
      "Epoch 29/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3688 - accuracy: 0.8680 - val_loss: 0.3642 - val_accuracy: 0.8720\n",
      "Epoch 30/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3658 - accuracy: 0.8702 - val_loss: 0.3626 - val_accuracy: 0.8718\n",
      "Epoch 31/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3619 - accuracy: 0.8715 - val_loss: 0.3605 - val_accuracy: 0.8719\n",
      "Epoch 32/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3592 - accuracy: 0.8719 - val_loss: 0.3593 - val_accuracy: 0.8733\n",
      "Epoch 33/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3562 - accuracy: 0.8731 - val_loss: 0.3571 - val_accuracy: 0.8733\n",
      "Epoch 34/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3545 - accuracy: 0.8739 - val_loss: 0.3564 - val_accuracy: 0.8729\n",
      "Epoch 35/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3499 - accuracy: 0.8755 - val_loss: 0.3547 - val_accuracy: 0.8737\n",
      "Epoch 36/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3496 - accuracy: 0.8750 - val_loss: 0.3501 - val_accuracy: 0.8771\n",
      "Epoch 37/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3455 - accuracy: 0.8767 - val_loss: 0.3499 - val_accuracy: 0.8762\n",
      "Epoch 38/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3446 - accuracy: 0.8756 - val_loss: 0.3472 - val_accuracy: 0.8762\n",
      "Epoch 39/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3422 - accuracy: 0.8778 - val_loss: 0.3476 - val_accuracy: 0.8773\n",
      "Epoch 40/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3412 - accuracy: 0.8773 - val_loss: 0.3453 - val_accuracy: 0.8781\n",
      "Epoch 41/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3377 - accuracy: 0.8792 - val_loss: 0.3479 - val_accuracy: 0.8773\n",
      "Epoch 42/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3339 - accuracy: 0.8808 - val_loss: 0.3429 - val_accuracy: 0.8787\n",
      "Epoch 43/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3322 - accuracy: 0.8814 - val_loss: 0.3418 - val_accuracy: 0.8786\n",
      "Epoch 44/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3306 - accuracy: 0.8817 - val_loss: 0.3398 - val_accuracy: 0.8790\n",
      "Epoch 45/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3274 - accuracy: 0.8830 - val_loss: 0.3425 - val_accuracy: 0.8787\n",
      "Epoch 46/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3273 - accuracy: 0.8823 - val_loss: 0.3362 - val_accuracy: 0.8804\n",
      "Epoch 47/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3241 - accuracy: 0.8835 - val_loss: 0.3370 - val_accuracy: 0.8806\n",
      "Epoch 48/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3221 - accuracy: 0.8854 - val_loss: 0.3364 - val_accuracy: 0.8792\n",
      "Epoch 49/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3198 - accuracy: 0.8857 - val_loss: 0.3335 - val_accuracy: 0.8813\n",
      "Epoch 50/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3179 - accuracy: 0.8857 - val_loss: 0.3365 - val_accuracy: 0.8811\n",
      "Epoch 51/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3163 - accuracy: 0.8877 - val_loss: 0.3304 - val_accuracy: 0.8833\n",
      "Epoch 52/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3124 - accuracy: 0.8881 - val_loss: 0.3295 - val_accuracy: 0.8830\n",
      "Epoch 53/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3126 - accuracy: 0.8878 - val_loss: 0.3332 - val_accuracy: 0.8818\n",
      "Epoch 54/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3124 - accuracy: 0.8871 - val_loss: 0.3275 - val_accuracy: 0.8832\n",
      "Epoch 55/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3077 - accuracy: 0.8889 - val_loss: 0.3284 - val_accuracy: 0.8827\n",
      "Epoch 56/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3070 - accuracy: 0.8902 - val_loss: 0.3268 - val_accuracy: 0.8839\n",
      "Epoch 57/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3066 - accuracy: 0.8913 - val_loss: 0.3243 - val_accuracy: 0.8844\n",
      "Epoch 58/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3031 - accuracy: 0.8911 - val_loss: 0.3236 - val_accuracy: 0.8848\n",
      "Epoch 59/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3019 - accuracy: 0.8924 - val_loss: 0.3236 - val_accuracy: 0.8855\n",
      "Epoch 60/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3002 - accuracy: 0.8929 - val_loss: 0.3224 - val_accuracy: 0.8857\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3466 - accuracy: 0.8761\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL1 : Drop out= 0.2 학습 \\n\")\n",
    "with tf.device('/cpu:0'):\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model1.add(Dropout(0.2))\n",
    "    #model.add(BatchNormalization())\n",
    "    model1.add(Dense(512, activation='relu'))\n",
    "    model1.add(Dropout(0.2))\n",
    "    #model.add(BatchNormalization())\n",
    "    model1.add(Dense(num_classes, activation='softmax'))\n",
    "    model1.summary()\n",
    "    model1.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "history = model1.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)\n",
    "metrics1 = model1.evaluate(x_test, y_test) #returns loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. MODEL2 : Drop out= 0.5 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL2 : Drop out= 0.5 학습 \n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 1.4242 - accuracy: 0.5076 - val_loss: 0.8076 - val_accuracy: 0.7228\n",
      "Epoch 2/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.8997 - accuracy: 0.6822 - val_loss: 0.6741 - val_accuracy: 0.7624\n",
      "Epoch 3/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.7752 - accuracy: 0.7298 - val_loss: 0.6133 - val_accuracy: 0.7813\n",
      "Epoch 4/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.7042 - accuracy: 0.7534 - val_loss: 0.5728 - val_accuracy: 0.7987\n",
      "Epoch 5/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.6601 - accuracy: 0.7723 - val_loss: 0.5393 - val_accuracy: 0.8112\n",
      "Epoch 6/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.6249 - accuracy: 0.7847 - val_loss: 0.5183 - val_accuracy: 0.8192\n",
      "Epoch 7/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5964 - accuracy: 0.7900 - val_loss: 0.5002 - val_accuracy: 0.8245\n",
      "Epoch 8/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5759 - accuracy: 0.8000 - val_loss: 0.4854 - val_accuracy: 0.8302\n",
      "Epoch 9/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5583 - accuracy: 0.8062 - val_loss: 0.4757 - val_accuracy: 0.8299\n",
      "Epoch 10/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5437 - accuracy: 0.8105 - val_loss: 0.4640 - val_accuracy: 0.8348\n",
      "Epoch 11/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5299 - accuracy: 0.8142 - val_loss: 0.4564 - val_accuracy: 0.8380\n",
      "Epoch 12/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5163 - accuracy: 0.8195 - val_loss: 0.4477 - val_accuracy: 0.8401\n",
      "Epoch 13/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5105 - accuracy: 0.8211 - val_loss: 0.4412 - val_accuracy: 0.8417\n",
      "Epoch 14/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4991 - accuracy: 0.8248 - val_loss: 0.4335 - val_accuracy: 0.8449\n",
      "Epoch 15/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4900 - accuracy: 0.8282 - val_loss: 0.4258 - val_accuracy: 0.8468\n",
      "Epoch 16/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4814 - accuracy: 0.8301 - val_loss: 0.4233 - val_accuracy: 0.8498\n",
      "Epoch 17/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4737 - accuracy: 0.8343 - val_loss: 0.4174 - val_accuracy: 0.8500\n",
      "Epoch 18/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.4675 - accuracy: 0.8360 - val_loss: 0.4137 - val_accuracy: 0.8520\n",
      "Epoch 19/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4652 - accuracy: 0.8365 - val_loss: 0.4076 - val_accuracy: 0.8542\n",
      "Epoch 20/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4619 - accuracy: 0.8359 - val_loss: 0.4032 - val_accuracy: 0.8549\n",
      "Epoch 21/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4528 - accuracy: 0.8404 - val_loss: 0.4023 - val_accuracy: 0.8556\n",
      "Epoch 22/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.4486 - accuracy: 0.8409 - val_loss: 0.3989 - val_accuracy: 0.8563\n",
      "Epoch 23/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4429 - accuracy: 0.8441 - val_loss: 0.3963 - val_accuracy: 0.8579\n",
      "Epoch 24/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4391 - accuracy: 0.8439 - val_loss: 0.3905 - val_accuracy: 0.8586\n",
      "Epoch 25/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.4359 - accuracy: 0.8459 - val_loss: 0.3876 - val_accuracy: 0.8602\n",
      "Epoch 26/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.4302 - accuracy: 0.8481 - val_loss: 0.3853 - val_accuracy: 0.8626\n",
      "Epoch 27/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4239 - accuracy: 0.8487 - val_loss: 0.3825 - val_accuracy: 0.8622\n",
      "Epoch 28/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4239 - accuracy: 0.8509 - val_loss: 0.3812 - val_accuracy: 0.8636\n",
      "Epoch 29/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4199 - accuracy: 0.8510 - val_loss: 0.3770 - val_accuracy: 0.8644\n",
      "Epoch 30/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4162 - accuracy: 0.8521 - val_loss: 0.3779 - val_accuracy: 0.8623\n",
      "Epoch 31/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4120 - accuracy: 0.8541 - val_loss: 0.3728 - val_accuracy: 0.8648\n",
      "Epoch 32/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4103 - accuracy: 0.8546 - val_loss: 0.3736 - val_accuracy: 0.8651\n",
      "Epoch 33/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.4076 - accuracy: 0.8537 - val_loss: 0.3705 - val_accuracy: 0.8668\n",
      "Epoch 34/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4042 - accuracy: 0.8564 - val_loss: 0.3686 - val_accuracy: 0.8670\n",
      "Epoch 35/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4018 - accuracy: 0.8580 - val_loss: 0.3674 - val_accuracy: 0.8683\n",
      "Epoch 36/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.4005 - accuracy: 0.8584 - val_loss: 0.3628 - val_accuracy: 0.8692\n",
      "Epoch 37/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3965 - accuracy: 0.8580 - val_loss: 0.3625 - val_accuracy: 0.8693\n",
      "Epoch 38/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3923 - accuracy: 0.8605 - val_loss: 0.3596 - val_accuracy: 0.8699\n",
      "Epoch 39/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3915 - accuracy: 0.8608 - val_loss: 0.3593 - val_accuracy: 0.8713\n",
      "Epoch 40/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3897 - accuracy: 0.8622 - val_loss: 0.3578 - val_accuracy: 0.8716\n",
      "Epoch 41/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3851 - accuracy: 0.8644 - val_loss: 0.3581 - val_accuracy: 0.8708\n",
      "Epoch 42/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3826 - accuracy: 0.8654 - val_loss: 0.3544 - val_accuracy: 0.8722\n",
      "Epoch 43/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3827 - accuracy: 0.8648 - val_loss: 0.3529 - val_accuracy: 0.8726\n",
      "Epoch 44/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3828 - accuracy: 0.8642 - val_loss: 0.3508 - val_accuracy: 0.8737\n",
      "Epoch 45/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3777 - accuracy: 0.8656 - val_loss: 0.3496 - val_accuracy: 0.8749\n",
      "Epoch 46/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3747 - accuracy: 0.8670 - val_loss: 0.3484 - val_accuracy: 0.8749\n",
      "Epoch 47/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3739 - accuracy: 0.8657 - val_loss: 0.3487 - val_accuracy: 0.8749\n",
      "Epoch 48/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3708 - accuracy: 0.8666 - val_loss: 0.3463 - val_accuracy: 0.8755\n",
      "Epoch 49/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3726 - accuracy: 0.8675 - val_loss: 0.3450 - val_accuracy: 0.8761\n",
      "Epoch 50/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.3691 - accuracy: 0.8696 - val_loss: 0.3467 - val_accuracy: 0.8760\n",
      "Epoch 51/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3658 - accuracy: 0.8696 - val_loss: 0.3420 - val_accuracy: 0.8769\n",
      "Epoch 52/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3665 - accuracy: 0.8703 - val_loss: 0.3418 - val_accuracy: 0.8763\n",
      "Epoch 53/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3603 - accuracy: 0.8706 - val_loss: 0.3404 - val_accuracy: 0.8761\n",
      "Epoch 54/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3582 - accuracy: 0.8720 - val_loss: 0.3401 - val_accuracy: 0.8764\n",
      "Epoch 55/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3586 - accuracy: 0.8720 - val_loss: 0.3394 - val_accuracy: 0.8765\n",
      "Epoch 56/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3572 - accuracy: 0.8733 - val_loss: 0.3376 - val_accuracy: 0.8771\n",
      "Epoch 57/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3548 - accuracy: 0.8750 - val_loss: 0.3365 - val_accuracy: 0.8778\n",
      "Epoch 58/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3546 - accuracy: 0.8721 - val_loss: 0.3351 - val_accuracy: 0.8791\n",
      "Epoch 59/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3548 - accuracy: 0.8731 - val_loss: 0.3349 - val_accuracy: 0.8788\n",
      "Epoch 60/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.3507 - accuracy: 0.8748 - val_loss: 0.3335 - val_accuracy: 0.8804\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3618 - accuracy: 0.8705\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL2 : Drop out= 0.5 학습 \\n\")\n",
    "with tf.device('/cpu:1'):\n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model2.add(Dropout(0.5))\n",
    "    #model.add(BatchNormalization())\n",
    "    model2.add(Dense(512, activation='relu'))\n",
    "    model2.add(Dropout(0.5))\n",
    "    #model.add(BatchNormalization())\n",
    "    model2.add(Dense(num_classes, activation='softmax'))\n",
    "    model2.summary()\n",
    "    model2.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "history = model2.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)\n",
    "metrics2 = model2.evaluate(x_test, y_test) #returns loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. MODEL3 : Drop out= 0.8 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL3 : Drop out= 0.8 학습 \n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "375/375 [==============================] - 7s 17ms/step - loss: 1.9802 - accuracy: 0.2981 - val_loss: 1.2360 - val_accuracy: 0.6743\n",
      "Epoch 2/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.3864 - accuracy: 0.4942 - val_loss: 0.8993 - val_accuracy: 0.7020\n",
      "Epoch 3/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 1.1488 - accuracy: 0.5750 - val_loss: 0.7811 - val_accuracy: 0.7172\n",
      "Epoch 4/60\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 1.0306 - accuracy: 0.6193 - val_loss: 0.7262 - val_accuracy: 0.7379\n",
      "Epoch 5/60\n",
      "375/375 [==============================] - 7s 19ms/step - loss: 0.9507 - accuracy: 0.6475 - val_loss: 0.6806 - val_accuracy: 0.7613\n",
      "Epoch 6/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.8940 - accuracy: 0.6706 - val_loss: 0.6534 - val_accuracy: 0.7631\n",
      "Epoch 7/60\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.8535 - accuracy: 0.6860 - val_loss: 0.6239 - val_accuracy: 0.7792\n",
      "Epoch 8/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.8175 - accuracy: 0.7025 - val_loss: 0.6049 - val_accuracy: 0.7837\n",
      "Epoch 9/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.7878 - accuracy: 0.7145 - val_loss: 0.5883 - val_accuracy: 0.7931\n",
      "Epoch 10/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.7615 - accuracy: 0.7236 - val_loss: 0.5747 - val_accuracy: 0.7963\n",
      "Epoch 11/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.7388 - accuracy: 0.7336 - val_loss: 0.5565 - val_accuracy: 0.8027\n",
      "Epoch 12/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.7228 - accuracy: 0.7389 - val_loss: 0.5440 - val_accuracy: 0.8063\n",
      "Epoch 13/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.7073 - accuracy: 0.7445 - val_loss: 0.5340 - val_accuracy: 0.8106\n",
      "Epoch 14/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6908 - accuracy: 0.7527 - val_loss: 0.5242 - val_accuracy: 0.8163\n",
      "Epoch 15/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.6812 - accuracy: 0.7574 - val_loss: 0.5167 - val_accuracy: 0.8208\n",
      "Epoch 16/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.6667 - accuracy: 0.7634 - val_loss: 0.5108 - val_accuracy: 0.8219\n",
      "Epoch 17/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.6598 - accuracy: 0.7666 - val_loss: 0.5007 - val_accuracy: 0.8248\n",
      "Epoch 18/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.6457 - accuracy: 0.7713 - val_loss: 0.4953 - val_accuracy: 0.8273\n",
      "Epoch 19/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.6368 - accuracy: 0.7744 - val_loss: 0.4860 - val_accuracy: 0.8294\n",
      "Epoch 20/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.6286 - accuracy: 0.7780 - val_loss: 0.4827 - val_accuracy: 0.8327\n",
      "Epoch 21/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.6244 - accuracy: 0.7803 - val_loss: 0.4766 - val_accuracy: 0.8303\n",
      "Epoch 22/60\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.6150 - accuracy: 0.7837 - val_loss: 0.4754 - val_accuracy: 0.8342\n",
      "Epoch 23/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.6072 - accuracy: 0.7868 - val_loss: 0.4665 - val_accuracy: 0.8334\n",
      "Epoch 24/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6030 - accuracy: 0.7888 - val_loss: 0.4648 - val_accuracy: 0.8360\n",
      "Epoch 25/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5967 - accuracy: 0.7910 - val_loss: 0.4604 - val_accuracy: 0.8377\n",
      "Epoch 26/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5860 - accuracy: 0.7939 - val_loss: 0.4563 - val_accuracy: 0.8366\n",
      "Epoch 27/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5916 - accuracy: 0.7959 - val_loss: 0.4537 - val_accuracy: 0.8395\n",
      "Epoch 28/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5772 - accuracy: 0.7985 - val_loss: 0.4532 - val_accuracy: 0.8407\n",
      "Epoch 29/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5747 - accuracy: 0.8000 - val_loss: 0.4470 - val_accuracy: 0.8423\n",
      "Epoch 30/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5706 - accuracy: 0.8002 - val_loss: 0.4469 - val_accuracy: 0.8420\n",
      "Epoch 31/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5678 - accuracy: 0.8015 - val_loss: 0.4440 - val_accuracy: 0.8435\n",
      "Epoch 32/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.5607 - accuracy: 0.8044 - val_loss: 0.4389 - val_accuracy: 0.8429\n",
      "Epoch 33/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5565 - accuracy: 0.8046 - val_loss: 0.4382 - val_accuracy: 0.8447\n",
      "Epoch 34/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5533 - accuracy: 0.8093 - val_loss: 0.4346 - val_accuracy: 0.8467\n",
      "Epoch 35/60\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.5472 - accuracy: 0.8086 - val_loss: 0.4313 - val_accuracy: 0.8469\n",
      "Epoch 36/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5484 - accuracy: 0.8097 - val_loss: 0.4294 - val_accuracy: 0.8472\n",
      "Epoch 37/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5435 - accuracy: 0.8108 - val_loss: 0.4279 - val_accuracy: 0.8470\n",
      "Epoch 38/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5395 - accuracy: 0.8115 - val_loss: 0.4263 - val_accuracy: 0.8486\n",
      "Epoch 39/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5391 - accuracy: 0.8140 - val_loss: 0.4257 - val_accuracy: 0.8497\n",
      "Epoch 40/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5343 - accuracy: 0.8154 - val_loss: 0.4247 - val_accuracy: 0.8501\n",
      "Epoch 41/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5306 - accuracy: 0.8154 - val_loss: 0.4192 - val_accuracy: 0.8498\n",
      "Epoch 42/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5258 - accuracy: 0.8165 - val_loss: 0.4206 - val_accuracy: 0.8495\n",
      "Epoch 43/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5220 - accuracy: 0.8187 - val_loss: 0.4176 - val_accuracy: 0.8525\n",
      "Epoch 44/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5243 - accuracy: 0.8186 - val_loss: 0.4195 - val_accuracy: 0.8518\n",
      "Epoch 45/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5171 - accuracy: 0.8199 - val_loss: 0.4126 - val_accuracy: 0.8528\n",
      "Epoch 46/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5141 - accuracy: 0.8196 - val_loss: 0.4136 - val_accuracy: 0.8542\n",
      "Epoch 47/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5155 - accuracy: 0.8212 - val_loss: 0.4099 - val_accuracy: 0.8542\n",
      "Epoch 48/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5093 - accuracy: 0.8226 - val_loss: 0.4106 - val_accuracy: 0.8538\n",
      "Epoch 49/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5106 - accuracy: 0.8236 - val_loss: 0.4090 - val_accuracy: 0.8553\n",
      "Epoch 50/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5024 - accuracy: 0.8254 - val_loss: 0.4085 - val_accuracy: 0.8566\n",
      "Epoch 51/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5044 - accuracy: 0.8238 - val_loss: 0.4067 - val_accuracy: 0.8558\n",
      "Epoch 52/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5031 - accuracy: 0.8246 - val_loss: 0.4062 - val_accuracy: 0.8562\n",
      "Epoch 53/60\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.5005 - accuracy: 0.8263 - val_loss: 0.4031 - val_accuracy: 0.8562\n",
      "Epoch 54/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4957 - accuracy: 0.8269 - val_loss: 0.4012 - val_accuracy: 0.8586\n",
      "Epoch 55/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4954 - accuracy: 0.8269 - val_loss: 0.4007 - val_accuracy: 0.8579\n",
      "Epoch 56/60\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.4953 - accuracy: 0.8282 - val_loss: 0.3999 - val_accuracy: 0.8591\n",
      "Epoch 57/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.4899 - accuracy: 0.8305 - val_loss: 0.3977 - val_accuracy: 0.8586\n",
      "Epoch 58/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4917 - accuracy: 0.8289 - val_loss: 0.3983 - val_accuracy: 0.8599\n",
      "Epoch 59/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4880 - accuracy: 0.8304 - val_loss: 0.3966 - val_accuracy: 0.8613\n",
      "Epoch 60/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4901 - accuracy: 0.8303 - val_loss: 0.3973 - val_accuracy: 0.8592\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4184 - accuracy: 0.8511\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL3 : Drop out= 0.8 학습 \\n\")\n",
    "with tf.device('/cpu:0'):\n",
    "    model3 = Sequential()\n",
    "    model3.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model3.add(Dropout(0.8))\n",
    "    #model.add(BatchNormalization())\n",
    "    model3.add(Dense(512, activation='relu'))\n",
    "    model3.add(Dropout(0.8))\n",
    "    #model.add(BatchNormalization())\n",
    "    model3.add(Dense(num_classes, activation='softmax'))\n",
    "    model3.summary()\n",
    "    model3.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "history = model3.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "metrics3 = model3.evaluate(x_test, y_test) #returns loss and accuracy\n",
    "# print(metrics[1])\n",
    "# print(f'Accuracy: {metrics[1]*100:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 정확성 비교 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8761000037193298\n",
      "model1(drop out : 0.2 ) Accuracy: 87.61%\n",
      "\n",
      "0.8705000281333923\n",
      "model2(drop out : 0.5 ) Accuracy: 87.05%\n",
      "\n",
      "0.8511000275611877\n",
      "model3(drop out : 0.8 ) Accuracy: 85.11%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics1[1])\n",
    "print(f'model1(drop out : 0.2 ) Accuracy: {metrics1[1]*100:.2f}%\\n')\n",
    "print(metrics2[1])\n",
    "print(f'model2(drop out : 0.5 ) Accuracy: {metrics2[1]*100:.2f}%\\n')\n",
    "print(metrics3[1])\n",
    "print(f'model3(drop out : 0.8 ) Accuracy: {metrics3[1]*100:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "\n",
    "\n",
    "### Drop out이란 \n",
    "- 모든 노드를 학습에 참여시키는 것이 아닌 특정 뉴런을 랜덤하게 제거해 학습하는 방식이다. \n",
    "- 특정 노드 위주로 학습되는 경향과 노드 간의 강한 연결을 없앨 수 있기 때문에 과적합을 방지하는데 쓰일 수 있다.\n",
    "\n",
    "\n",
    "#### <해석>\n",
    "이런 drop out에서는 사용자가 얼마나 많은 존속하는 지에 대한 하이퍼파라미터 p를 설정할 수 있는데 <br>\n",
    "우리는 각각 0.2, 0.5, 0.8이라는 값을 설정하여 각 모델들을 학습했다.  \n",
    "\n",
    "</p>\n",
    "\n",
    "각 drop out 비율에 따른 정확도는 다음과 같다  \n",
    "> model1(drop out : 0.2 ) Accuracy: 87.61%  \n",
    "> model2(drop out : 0.5 ) Accuracy: 87.05%  \n",
    "> model3(drop out : 0.8 ) Accuracy: 85.11%\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size : 15px\">\n",
    "결과는 보면 drop out 비율이 높아짐에 따라 점점 정확도가 줄고 있는 것을 확인할 수 있다.<br>\n",
    "이는 점점 학습에 참여하는 노드의 수가 줄어듦에 따라 모델의 정보 손실이 그만큼 더 많이 일어났다는 뜻으로 볼 수 있다. <br>\n",
    "즉, drop out 비율을 높이는 것은 overfitting의 방지에 도움이 될 수 있으나 그만큼 모델이 underfitting이 일어날 가능성이 높아진다는 것을 의미한다. <br>\n",
    "따라서 적절한 parameter를 설정하는 것이 좋다. 해당 데이터에서는 0.2가 가장 적합하여 보인다.<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL4 : Drop out= 0.2, batch BatchNormalization \n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 673,802\n",
      "Trainable params: 671,754\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "375/375 [==============================] - 6s 14ms/step - loss: 0.7369 - accuracy: 0.7444 - val_loss: 0.5067 - val_accuracy: 0.8238\n",
      "Epoch 2/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5156 - accuracy: 0.8179 - val_loss: 0.4190 - val_accuracy: 0.8479\n",
      "Epoch 3/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4571 - accuracy: 0.8379 - val_loss: 0.3898 - val_accuracy: 0.8604\n",
      "Epoch 4/60\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.4308 - accuracy: 0.8449 - val_loss: 0.3727 - val_accuracy: 0.8676\n",
      "Epoch 5/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4075 - accuracy: 0.8535 - val_loss: 0.3623 - val_accuracy: 0.8711\n",
      "Epoch 6/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3906 - accuracy: 0.8598 - val_loss: 0.3550 - val_accuracy: 0.8710\n",
      "Epoch 7/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3741 - accuracy: 0.8644 - val_loss: 0.3475 - val_accuracy: 0.8730\n",
      "Epoch 8/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3632 - accuracy: 0.8688 - val_loss: 0.3486 - val_accuracy: 0.8737\n",
      "Epoch 9/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3513 - accuracy: 0.8714 - val_loss: 0.3367 - val_accuracy: 0.8777\n",
      "Epoch 10/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3399 - accuracy: 0.8768 - val_loss: 0.3323 - val_accuracy: 0.8808\n",
      "Epoch 11/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.3318 - accuracy: 0.8804 - val_loss: 0.3286 - val_accuracy: 0.8811\n",
      "Epoch 12/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.3234 - accuracy: 0.8819 - val_loss: 0.3297 - val_accuracy: 0.8792\n",
      "Epoch 13/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3185 - accuracy: 0.8845 - val_loss: 0.3246 - val_accuracy: 0.8822\n",
      "Epoch 14/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.3099 - accuracy: 0.8867 - val_loss: 0.3225 - val_accuracy: 0.8837\n",
      "Epoch 15/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.3021 - accuracy: 0.8895 - val_loss: 0.3180 - val_accuracy: 0.8856\n",
      "Epoch 16/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2998 - accuracy: 0.8906 - val_loss: 0.3160 - val_accuracy: 0.8852\n",
      "Epoch 17/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2949 - accuracy: 0.8911 - val_loss: 0.3158 - val_accuracy: 0.8838\n",
      "Epoch 18/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2839 - accuracy: 0.8969 - val_loss: 0.3175 - val_accuracy: 0.8829\n",
      "Epoch 19/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.2811 - accuracy: 0.8956 - val_loss: 0.3110 - val_accuracy: 0.8864\n",
      "Epoch 20/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.2761 - accuracy: 0.8979 - val_loss: 0.3098 - val_accuracy: 0.8858\n",
      "Epoch 21/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2708 - accuracy: 0.9005 - val_loss: 0.3099 - val_accuracy: 0.8864\n",
      "Epoch 22/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.2665 - accuracy: 0.9022 - val_loss: 0.3164 - val_accuracy: 0.8842\n",
      "Epoch 23/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.2607 - accuracy: 0.9037 - val_loss: 0.3091 - val_accuracy: 0.8885\n",
      "Epoch 24/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2574 - accuracy: 0.9065 - val_loss: 0.3077 - val_accuracy: 0.8863\n",
      "Epoch 25/60\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.2544 - accuracy: 0.9062 - val_loss: 0.3067 - val_accuracy: 0.8900\n",
      "Epoch 26/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.2511 - accuracy: 0.9064 - val_loss: 0.3055 - val_accuracy: 0.8897\n",
      "Epoch 27/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.2462 - accuracy: 0.9093 - val_loss: 0.3065 - val_accuracy: 0.8893\n",
      "Epoch 28/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.2416 - accuracy: 0.9119 - val_loss: 0.3041 - val_accuracy: 0.8895\n",
      "Epoch 29/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.2386 - accuracy: 0.9115 - val_loss: 0.3016 - val_accuracy: 0.8907\n",
      "Epoch 30/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2330 - accuracy: 0.9139 - val_loss: 0.2993 - val_accuracy: 0.8920\n",
      "Epoch 31/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2332 - accuracy: 0.9127 - val_loss: 0.3056 - val_accuracy: 0.8895\n",
      "Epoch 32/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2286 - accuracy: 0.9149 - val_loss: 0.3055 - val_accuracy: 0.8914\n",
      "Epoch 33/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2237 - accuracy: 0.9179 - val_loss: 0.3007 - val_accuracy: 0.8933\n",
      "Epoch 34/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2209 - accuracy: 0.9187 - val_loss: 0.3036 - val_accuracy: 0.8905\n",
      "Epoch 35/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2221 - accuracy: 0.9171 - val_loss: 0.3038 - val_accuracy: 0.8903\n",
      "Epoch 36/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.2188 - accuracy: 0.9194 - val_loss: 0.3010 - val_accuracy: 0.8939\n",
      "Epoch 37/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.2098 - accuracy: 0.9220 - val_loss: 0.3003 - val_accuracy: 0.8907\n",
      "Epoch 38/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2109 - accuracy: 0.9206 - val_loss: 0.3042 - val_accuracy: 0.8923\n",
      "Epoch 39/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.2062 - accuracy: 0.9244 - val_loss: 0.3081 - val_accuracy: 0.8938\n",
      "Epoch 40/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.2046 - accuracy: 0.9236 - val_loss: 0.3046 - val_accuracy: 0.8923\n",
      "Epoch 41/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.2010 - accuracy: 0.9240 - val_loss: 0.2996 - val_accuracy: 0.8942\n",
      "Epoch 42/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.1991 - accuracy: 0.9265 - val_loss: 0.3065 - val_accuracy: 0.8917\n",
      "Epoch 43/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.1959 - accuracy: 0.9264 - val_loss: 0.3063 - val_accuracy: 0.8932\n",
      "Epoch 44/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.1939 - accuracy: 0.9277 - val_loss: 0.3113 - val_accuracy: 0.8925\n",
      "Epoch 45/60\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.1900 - accuracy: 0.9294 - val_loss: 0.3222 - val_accuracy: 0.8890\n",
      "Epoch 46/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.1887 - accuracy: 0.9299 - val_loss: 0.3072 - val_accuracy: 0.8932\n",
      "Epoch 47/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1844 - accuracy: 0.9315 - val_loss: 0.3077 - val_accuracy: 0.8941\n",
      "Epoch 48/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.1828 - accuracy: 0.9330 - val_loss: 0.3291 - val_accuracy: 0.8871\n",
      "Epoch 49/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.1781 - accuracy: 0.9329 - val_loss: 0.3082 - val_accuracy: 0.8934\n",
      "Epoch 50/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.1756 - accuracy: 0.9354 - val_loss: 0.3132 - val_accuracy: 0.8904\n",
      "Epoch 51/60\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.1757 - accuracy: 0.9348 - val_loss: 0.3146 - val_accuracy: 0.8948\n",
      "Epoch 52/60\n",
      "375/375 [==============================] - 7s 19ms/step - loss: 0.1756 - accuracy: 0.9343 - val_loss: 0.3075 - val_accuracy: 0.8932\n",
      "Epoch 53/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.1691 - accuracy: 0.9381 - val_loss: 0.3141 - val_accuracy: 0.8938\n",
      "Epoch 54/60\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.1688 - accuracy: 0.9370 - val_loss: 0.3131 - val_accuracy: 0.8950\n",
      "Epoch 55/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.1664 - accuracy: 0.9387 - val_loss: 0.3187 - val_accuracy: 0.8948\n",
      "Epoch 56/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.1652 - accuracy: 0.9397 - val_loss: 0.3144 - val_accuracy: 0.8957\n",
      "Epoch 57/60\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.1638 - accuracy: 0.9393 - val_loss: 0.3140 - val_accuracy: 0.8954\n",
      "Epoch 58/60\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1612 - accuracy: 0.9403 - val_loss: 0.3091 - val_accuracy: 0.8949\n",
      "Epoch 59/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1569 - accuracy: 0.9427 - val_loss: 0.3137 - val_accuracy: 0.8939\n",
      "Epoch 60/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.1578 - accuracy: 0.9411 - val_loss: 0.3187 - val_accuracy: 0.8955\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3459 - accuracy: 0.8892\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL4 : Drop out= 0.2, batch BatchNormalization \\n\")\n",
    "with tf.device('/cpu:0'):\n",
    "    model4 = Sequential()\n",
    "    model4.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model4.add(Dropout(0.2))\n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(Dense(512, activation='relu'))\n",
    "    model4.add(Dropout(0.2))\n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(Dense(num_classes, activation='softmax'))\n",
    "    model4.summary()\n",
    "    model4.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "history = model4.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)\n",
    "metrics4 = model4.evaluate(x_test, y_test) #returns loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL5 : Drop out= 0.5, batch BatchNormalization \n",
      "\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 673,802\n",
      "Trainable params: 671,754\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 1.0519 - accuracy: 0.6414 - val_loss: 0.6073 - val_accuracy: 0.7919\n",
      "Epoch 2/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6940 - accuracy: 0.7541 - val_loss: 0.4941 - val_accuracy: 0.8226\n",
      "Epoch 3/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.6123 - accuracy: 0.7841 - val_loss: 0.4574 - val_accuracy: 0.8359\n",
      "Epoch 4/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5674 - accuracy: 0.7987 - val_loss: 0.4354 - val_accuracy: 0.8428\n",
      "Epoch 5/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5338 - accuracy: 0.8104 - val_loss: 0.4220 - val_accuracy: 0.8503\n",
      "Epoch 6/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5145 - accuracy: 0.8162 - val_loss: 0.4110 - val_accuracy: 0.8534\n",
      "Epoch 7/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4949 - accuracy: 0.8236 - val_loss: 0.4018 - val_accuracy: 0.8568\n",
      "Epoch 8/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4815 - accuracy: 0.8286 - val_loss: 0.3968 - val_accuracy: 0.8560\n",
      "Epoch 9/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4690 - accuracy: 0.8314 - val_loss: 0.3932 - val_accuracy: 0.8580\n",
      "Epoch 10/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4576 - accuracy: 0.8368 - val_loss: 0.3870 - val_accuracy: 0.8589\n",
      "Epoch 11/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4459 - accuracy: 0.8404 - val_loss: 0.3789 - val_accuracy: 0.8627\n",
      "Epoch 12/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4379 - accuracy: 0.8429 - val_loss: 0.3760 - val_accuracy: 0.8643\n",
      "Epoch 13/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4295 - accuracy: 0.8463 - val_loss: 0.3689 - val_accuracy: 0.8654\n",
      "Epoch 14/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4235 - accuracy: 0.8480 - val_loss: 0.3690 - val_accuracy: 0.8679\n",
      "Epoch 15/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4181 - accuracy: 0.8506 - val_loss: 0.3626 - val_accuracy: 0.8693\n",
      "Epoch 16/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4099 - accuracy: 0.8519 - val_loss: 0.3639 - val_accuracy: 0.8689\n",
      "Epoch 17/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4078 - accuracy: 0.8532 - val_loss: 0.3617 - val_accuracy: 0.8702\n",
      "Epoch 18/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4030 - accuracy: 0.8563 - val_loss: 0.3542 - val_accuracy: 0.8748\n",
      "Epoch 19/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3939 - accuracy: 0.8574 - val_loss: 0.3514 - val_accuracy: 0.8734\n",
      "Epoch 20/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3935 - accuracy: 0.8576 - val_loss: 0.3485 - val_accuracy: 0.8735\n",
      "Epoch 21/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3845 - accuracy: 0.8616 - val_loss: 0.3505 - val_accuracy: 0.8741\n",
      "Epoch 22/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3859 - accuracy: 0.8608 - val_loss: 0.3468 - val_accuracy: 0.8748\n",
      "Epoch 23/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.3779 - accuracy: 0.8645 - val_loss: 0.3468 - val_accuracy: 0.8756\n",
      "Epoch 24/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3747 - accuracy: 0.8651 - val_loss: 0.3404 - val_accuracy: 0.8770\n",
      "Epoch 25/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.3689 - accuracy: 0.8674 - val_loss: 0.3438 - val_accuracy: 0.8777\n",
      "Epoch 26/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.3700 - accuracy: 0.8653 - val_loss: 0.3389 - val_accuracy: 0.8794\n",
      "Epoch 27/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.3632 - accuracy: 0.8687 - val_loss: 0.3391 - val_accuracy: 0.8797\n",
      "Epoch 28/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3648 - accuracy: 0.8672 - val_loss: 0.3346 - val_accuracy: 0.8791\n",
      "Epoch 29/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3595 - accuracy: 0.8694 - val_loss: 0.3337 - val_accuracy: 0.8815\n",
      "Epoch 30/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.3565 - accuracy: 0.8691 - val_loss: 0.3366 - val_accuracy: 0.8799\n",
      "Epoch 31/60\n",
      "375/375 [==============================] - 7s 19ms/step - loss: 0.3505 - accuracy: 0.8730 - val_loss: 0.3299 - val_accuracy: 0.8809\n",
      "Epoch 32/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3531 - accuracy: 0.8715 - val_loss: 0.3271 - val_accuracy: 0.8823\n",
      "Epoch 33/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3465 - accuracy: 0.8749 - val_loss: 0.3294 - val_accuracy: 0.8827\n",
      "Epoch 34/60\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.3431 - accuracy: 0.8756 - val_loss: 0.3280 - val_accuracy: 0.8805\n",
      "Epoch 35/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.3420 - accuracy: 0.8759 - val_loss: 0.3260 - val_accuracy: 0.8845\n",
      "Epoch 36/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.3409 - accuracy: 0.8756 - val_loss: 0.3262 - val_accuracy: 0.8833\n",
      "Epoch 37/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3376 - accuracy: 0.8769 - val_loss: 0.3288 - val_accuracy: 0.8813\n",
      "Epoch 38/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.3359 - accuracy: 0.8781 - val_loss: 0.3244 - val_accuracy: 0.8834\n",
      "Epoch 39/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.3325 - accuracy: 0.8788 - val_loss: 0.3246 - val_accuracy: 0.8847\n",
      "Epoch 40/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3318 - accuracy: 0.8801 - val_loss: 0.3226 - val_accuracy: 0.8832\n",
      "Epoch 41/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3296 - accuracy: 0.8797 - val_loss: 0.3221 - val_accuracy: 0.8844\n",
      "Epoch 42/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3271 - accuracy: 0.8818 - val_loss: 0.3175 - val_accuracy: 0.8855\n",
      "Epoch 43/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.3251 - accuracy: 0.8808 - val_loss: 0.3177 - val_accuracy: 0.8849\n",
      "Epoch 44/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.3230 - accuracy: 0.8834 - val_loss: 0.3212 - val_accuracy: 0.8829\n",
      "Epoch 45/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.3208 - accuracy: 0.8830 - val_loss: 0.3193 - val_accuracy: 0.8857\n",
      "Epoch 46/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.3215 - accuracy: 0.8818 - val_loss: 0.3202 - val_accuracy: 0.8850\n",
      "Epoch 47/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.3147 - accuracy: 0.8855 - val_loss: 0.3245 - val_accuracy: 0.8816\n",
      "Epoch 48/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3149 - accuracy: 0.8855 - val_loss: 0.3159 - val_accuracy: 0.8855\n",
      "Epoch 49/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3132 - accuracy: 0.8857 - val_loss: 0.3162 - val_accuracy: 0.8849\n",
      "Epoch 50/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.3141 - accuracy: 0.8844 - val_loss: 0.3150 - val_accuracy: 0.8872\n",
      "Epoch 51/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.3092 - accuracy: 0.8874 - val_loss: 0.3135 - val_accuracy: 0.8862\n",
      "Epoch 52/60\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.3095 - accuracy: 0.8873 - val_loss: 0.3138 - val_accuracy: 0.8828\n",
      "Epoch 53/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.3044 - accuracy: 0.8888 - val_loss: 0.3169 - val_accuracy: 0.8846\n",
      "Epoch 54/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.3056 - accuracy: 0.8884 - val_loss: 0.3141 - val_accuracy: 0.8866\n",
      "Epoch 55/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3054 - accuracy: 0.8882 - val_loss: 0.3182 - val_accuracy: 0.8857\n",
      "Epoch 56/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.3025 - accuracy: 0.8886 - val_loss: 0.3121 - val_accuracy: 0.8862\n",
      "Epoch 57/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.3022 - accuracy: 0.8891 - val_loss: 0.3119 - val_accuracy: 0.8869\n",
      "Epoch 58/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.2994 - accuracy: 0.8895 - val_loss: 0.3142 - val_accuracy: 0.8864\n",
      "Epoch 59/60\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.2945 - accuracy: 0.8914 - val_loss: 0.3095 - val_accuracy: 0.8873\n",
      "Epoch 60/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2965 - accuracy: 0.8922 - val_loss: 0.3104 - val_accuracy: 0.8882\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3346 - accuracy: 0.8808\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL5 : Drop out= 0.5, batch BatchNormalization \\n\")\n",
    "with tf.device('/cpu:0'):\n",
    "    model5 = Sequential()\n",
    "    model5.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model5.add(Dropout(0.5))\n",
    "    model5.add(BatchNormalization())\n",
    "    model5.add(Dense(512, activation='relu'))\n",
    "    model5.add(Dropout(0.5))\n",
    "    model5.add(BatchNormalization())\n",
    "    model5.add(Dense(num_classes, activation='softmax'))\n",
    "    model5.summary()\n",
    "    model5.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "history = model5.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)\n",
    "metrics5 = model5.evaluate(x_test, y_test) #returns loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL6 : Drop out= 0.8, batch BatchNormalization \n",
      "\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 1.9492 - accuracy: 0.3062 - val_loss: 1.1941 - val_accuracy: 0.6728\n",
      "Epoch 2/60\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 1.3667 - accuracy: 0.4981 - val_loss: 0.8855 - val_accuracy: 0.6993\n",
      "Epoch 3/60\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 1.1451 - accuracy: 0.5716 - val_loss: 0.7814 - val_accuracy: 0.7220\n",
      "Epoch 4/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 1.0241 - accuracy: 0.6147 - val_loss: 0.7275 - val_accuracy: 0.7331\n",
      "Epoch 5/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.9532 - accuracy: 0.6460 - val_loss: 0.6882 - val_accuracy: 0.7562\n",
      "Epoch 6/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.9010 - accuracy: 0.6627 - val_loss: 0.6569 - val_accuracy: 0.7596\n",
      "Epoch 7/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.8542 - accuracy: 0.6823 - val_loss: 0.6310 - val_accuracy: 0.7738\n",
      "Epoch 8/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.8216 - accuracy: 0.6971 - val_loss: 0.6121 - val_accuracy: 0.7775\n",
      "Epoch 9/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.7941 - accuracy: 0.7070 - val_loss: 0.5946 - val_accuracy: 0.7890\n",
      "Epoch 10/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.7729 - accuracy: 0.7171 - val_loss: 0.5779 - val_accuracy: 0.7925\n",
      "Epoch 11/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.7498 - accuracy: 0.7265 - val_loss: 0.5657 - val_accuracy: 0.7960\n",
      "Epoch 12/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.7316 - accuracy: 0.7340 - val_loss: 0.5498 - val_accuracy: 0.8046\n",
      "Epoch 13/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.7159 - accuracy: 0.7422 - val_loss: 0.5385 - val_accuracy: 0.8076\n",
      "Epoch 14/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6958 - accuracy: 0.7494 - val_loss: 0.5310 - val_accuracy: 0.8103\n",
      "Epoch 15/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6889 - accuracy: 0.7530 - val_loss: 0.5209 - val_accuracy: 0.8179\n",
      "Epoch 16/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6726 - accuracy: 0.7585 - val_loss: 0.5143 - val_accuracy: 0.8197\n",
      "Epoch 17/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6676 - accuracy: 0.7646 - val_loss: 0.5043 - val_accuracy: 0.8211\n",
      "Epoch 18/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6559 - accuracy: 0.7676 - val_loss: 0.4986 - val_accuracy: 0.8232\n",
      "Epoch 19/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6418 - accuracy: 0.7711 - val_loss: 0.4900 - val_accuracy: 0.8256\n",
      "Epoch 20/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6302 - accuracy: 0.7772 - val_loss: 0.4863 - val_accuracy: 0.8283\n",
      "Epoch 21/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6264 - accuracy: 0.7782 - val_loss: 0.4816 - val_accuracy: 0.8291\n",
      "Epoch 22/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6172 - accuracy: 0.7809 - val_loss: 0.4792 - val_accuracy: 0.8298\n",
      "Epoch 23/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.6123 - accuracy: 0.7831 - val_loss: 0.4712 - val_accuracy: 0.8328\n",
      "Epoch 24/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.6078 - accuracy: 0.7851 - val_loss: 0.4678 - val_accuracy: 0.8336\n",
      "Epoch 25/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.6007 - accuracy: 0.7892 - val_loss: 0.4660 - val_accuracy: 0.8342\n",
      "Epoch 26/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5939 - accuracy: 0.7922 - val_loss: 0.4590 - val_accuracy: 0.8373\n",
      "Epoch 27/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5914 - accuracy: 0.7933 - val_loss: 0.4550 - val_accuracy: 0.8373\n",
      "Epoch 28/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5841 - accuracy: 0.7977 - val_loss: 0.4565 - val_accuracy: 0.8367\n",
      "Epoch 29/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5827 - accuracy: 0.7957 - val_loss: 0.4514 - val_accuracy: 0.8403\n",
      "Epoch 30/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5744 - accuracy: 0.7990 - val_loss: 0.4478 - val_accuracy: 0.8389\n",
      "Epoch 31/60\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.5731 - accuracy: 0.7993 - val_loss: 0.4468 - val_accuracy: 0.8403\n",
      "Epoch 32/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5666 - accuracy: 0.8043 - val_loss: 0.4437 - val_accuracy: 0.8414\n",
      "Epoch 33/60\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.5618 - accuracy: 0.8033 - val_loss: 0.4415 - val_accuracy: 0.8418\n",
      "Epoch 34/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5619 - accuracy: 0.8036 - val_loss: 0.4389 - val_accuracy: 0.8452\n",
      "Epoch 35/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5537 - accuracy: 0.8066 - val_loss: 0.4349 - val_accuracy: 0.8444\n",
      "Epoch 36/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5468 - accuracy: 0.8100 - val_loss: 0.4338 - val_accuracy: 0.8450\n",
      "Epoch 37/60\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.5456 - accuracy: 0.8087 - val_loss: 0.4309 - val_accuracy: 0.8449\n",
      "Epoch 38/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5448 - accuracy: 0.8100 - val_loss: 0.4296 - val_accuracy: 0.8476\n",
      "Epoch 39/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5420 - accuracy: 0.8100 - val_loss: 0.4300 - val_accuracy: 0.8457\n",
      "Epoch 40/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5342 - accuracy: 0.8133 - val_loss: 0.4265 - val_accuracy: 0.8483\n",
      "Epoch 41/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5307 - accuracy: 0.8140 - val_loss: 0.4242 - val_accuracy: 0.8491\n",
      "Epoch 42/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.5305 - accuracy: 0.8146 - val_loss: 0.4208 - val_accuracy: 0.8485\n",
      "Epoch 43/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5337 - accuracy: 0.8158 - val_loss: 0.4196 - val_accuracy: 0.8496\n",
      "Epoch 44/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5312 - accuracy: 0.8139 - val_loss: 0.4198 - val_accuracy: 0.8509\n",
      "Epoch 45/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5253 - accuracy: 0.8173 - val_loss: 0.4163 - val_accuracy: 0.8503\n",
      "Epoch 46/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5215 - accuracy: 0.8198 - val_loss: 0.4166 - val_accuracy: 0.8512\n",
      "Epoch 47/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5183 - accuracy: 0.8197 - val_loss: 0.4133 - val_accuracy: 0.8503\n",
      "Epoch 48/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5162 - accuracy: 0.8212 - val_loss: 0.4129 - val_accuracy: 0.8518\n",
      "Epoch 49/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5112 - accuracy: 0.8239 - val_loss: 0.4128 - val_accuracy: 0.8548\n",
      "Epoch 50/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5067 - accuracy: 0.8244 - val_loss: 0.4113 - val_accuracy: 0.8544\n",
      "Epoch 51/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5081 - accuracy: 0.8244 - val_loss: 0.4084 - val_accuracy: 0.8544\n",
      "Epoch 52/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5090 - accuracy: 0.8227 - val_loss: 0.4064 - val_accuracy: 0.8547\n",
      "Epoch 53/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5047 - accuracy: 0.8235 - val_loss: 0.4109 - val_accuracy: 0.8524\n",
      "Epoch 54/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.5042 - accuracy: 0.8241 - val_loss: 0.4033 - val_accuracy: 0.8565\n",
      "Epoch 55/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4997 - accuracy: 0.8255 - val_loss: 0.4045 - val_accuracy: 0.8561\n",
      "Epoch 56/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4926 - accuracy: 0.8293 - val_loss: 0.4022 - val_accuracy: 0.8536\n",
      "Epoch 57/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4941 - accuracy: 0.8265 - val_loss: 0.4001 - val_accuracy: 0.8582\n",
      "Epoch 58/60\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.4939 - accuracy: 0.8268 - val_loss: 0.4002 - val_accuracy: 0.8553\n",
      "Epoch 59/60\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.4933 - accuracy: 0.8275 - val_loss: 0.4017 - val_accuracy: 0.8576\n",
      "Epoch 60/60\n",
      "375/375 [==============================] - 5s 15ms/step - loss: 0.4930 - accuracy: 0.8295 - val_loss: 0.3991 - val_accuracy: 0.8581\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4219 - accuracy: 0.8493\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL6 : Drop out= 0.8, batch BatchNormalization \\n\")\n",
    "with tf.device('/cpu:0'):\n",
    "    model6 = Sequential()\n",
    "    model6.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model6.add(Dropout(0.8))\n",
    "    #model.add(BatchNormalization())\n",
    "    model6.add(Dense(512, activation='relu'))\n",
    "    model6.add(Dropout(0.8))\n",
    "    #model.add(BatchNormalization())\n",
    "    model6.add(Dense(num_classes, activation='softmax'))\n",
    "    model6.summary()\n",
    "    model6.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "history = model6.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "metrics6 = model6.evaluate(x_test, y_test) #returns loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop out: 0.2\n",
      "model1 Accuracy: 87.61%\n",
      "model4(Batch normalization) Accuracy: 88.92%\n",
      "\n",
      "drop out: 0.5\n",
      "model2 Accuracy: 87.05%\n",
      "model5(Batch normalization) Accuracy: 88.08%\n",
      "\n",
      "drop out: 0.8\n",
      "model3 Accuracy: 85.11%\n",
      "model5(Batch normalization) Accuracy: 84.93%\n"
     ]
    }
   ],
   "source": [
    "# print(metrics4[1])\n",
    "# print(f'model4(Batch normalization) Accuracy: {metrics4[1]*100:.2f}%\\n')\n",
    "# print(metrics5[1])\n",
    "# print(f'model5(Batch normalization) Accuracy: {metrics5[1]*100:.2f}%\\n')\n",
    "# print(metrics6[1])\n",
    "# print(f'model5(Batch normalization) Accuracy: {metrics6[1]*100:.2f}%')\n",
    "print(\"drop out: 0.2\")\n",
    "print(f'model1 Accuracy: {metrics1[1]*100:.2f}%')\n",
    "print(f'model4(Batch normalization) Accuracy: {metrics4[1]*100:.2f}%\\n')\n",
    "\n",
    "\n",
    "print(\"drop out: 0.5\")\n",
    "print(f'model2 Accuracy: {metrics2[1]*100:.2f}%')\n",
    "print(f'model5(Batch normalization) Accuracy: {metrics5[1]*100:.2f}%\\n')\n",
    "\n",
    "print(\"drop out: 0.8\")\n",
    "print(f'model3 Accuracy: {metrics3[1]*100:.2f}%')\n",
    "print(f'model5(Batch normalization) Accuracy: {metrics6[1]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "\n",
    "\n",
    "### Batch Normalization이란 \n",
    "> 입력값이 양극단으로 간다면 미분 기울기가 0에 가까워져 vanishing gradient 문제에 빠질 수 있다.  \n",
    "> 만약 vanishing gradient가 일어난다면 학습량이 현저하게 줄어 레이어가 깊어질수록 에러를 잘 학습하지 못하게 되고 local minima에 빠질 우려가 있다.  \n",
    "> 이 때문에 우리는 정규화를 통해 학습 속도를 개선하고 정확성을 높이는 방법을 사용한다.  \n",
    "> 더욱이 Batch BatchNormalization을 통한다면 정규화된 값에 scale인 r(감마)와 shift인 b(베타)를 통해 원래의 값과 정규화된 값 사이의 값을 학습시킬 수 있다.   \n",
    "\n",
    "\n",
    "#### <해석>\n",
    "\n",
    "\n",
    "<p>\n",
    "\n",
    "앞서 본 drop out한 모델에 각각 Batch Normalization을 적용한 결과는 다음과 같다\n",
    "\n",
    "drop out: 0.2  \n",
    "> model 1 Accuracy: 87.61%  \n",
    "> model 4 (Batch normalization) Accuracy: 88.92%   \n",
    "   \n",
    "drop out: 0.5  \n",
    "> model 2 Accuracy: 87.05%  \n",
    "> model 5 (Batch normalization) Accuracy: 88.08%  \n",
    "\n",
    "drop out: 0.8  \n",
    "> model 3 Accuracy: 85.11%  \n",
    "> model 5 (Batch normalization) Accuracy: 84.93%  \n",
    "</p>\n",
    "\n",
    "<p style=\"font-size : 15px\">\n",
    "결과는 보면 drop 비율이 0.2, 0.5인 경우 정확도가 조금이나마 증가하는 것을 볼 수 있다.<br> 이는 정규화 과정을 통해 학습 속도 개선이 일어나 같은 iteration(60)에서 성능 향상이 일어났다는 것으로 볼 수 있다. <br> 하지만 비율이 0.8인 모델에서는 오히려 감소하는 경향을 보였는데 이는 drop out 비율이 높아짐에 따라 해당 모델이 undefitting되어 데이터를 잘 설명하지 못하기 때문이라 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsrElEQVR4nO3dd3xc1Z338c/RFJXRqMuWLNmWXHDvwiWmmJaYEsMCoQdMAs5DIAaS3Y3ZJw8BkuwT9sWyJK8lyYOJQ0Iw4EBYDDh0AyE2YBtcJXfLVu91RtK08/xxR7Iky5ZsjzS6M7/36zWvaVf3niuPvnN87ilKa40QQgjziwl3AYQQQoSGBLoQQkQICXQhhIgQEuhCCBEhJNCFECJCWMN14IyMDJ2XlxeuwwshhClt27atVmud2dd7YQv0vLw8tm7dGq7DCyGEKSmljp7sPWlyEUKICCGBLoQQEUICXQghIkTY2tD74vV6KS0tpb29PdxFiQhxcXHk5uZis9nCXRQhxBAYVoFeWlqK0+kkLy8PpVS4i2NqWmvq6uooLS0lPz8/3MURQgyBYdXk0t7eTnp6uoR5CCilSE9Pl//tCBFFhlWgAxLmISS/SyGiy7BqchFCiGGtoxUOvgdNZZA1HbJmQkLaidv5vdBQDPVHwNMKvnbwth2/n3ApjJod8uJJoHfT2NjI2rVr+f73v39aP3fFFVewdu1aUlJSTrrNww8/zAUXXMCll156lqUUIkJ1tELFDuhoBmssWGLBGgdWO8RYgV7/49QB8HuM8PR7jJuKgbRxkJwLvf+H2t4Mhz+Cg+/DkU/A12Hs2xoHluB90igYMRVGTjXuU/OgvQn2bYCiN+HQh+Dv6LnflDGQPQtSxhohXrsf6g9DwHfyc41LHpRAV+Fa4KKgoED3HilaVFTElClTwlIegOLiYq666ip2797d43Wfz4fVas7vvnD/ToU4QSAA7lpoLofKnVC6Fcq2QXWhEdKhYE+EjImQORmc2VDyBZR8ZoRsbBLkXwDxKeDzGLVmv8eoOTceM0KZYC5a4433tB+ScmHKVTDlm5AxCap2Q8V240uofDs0l0FqvnHcjHMgc5Lx5RLrNL4sbPHH7y32E79wBkgptU1rXdDXe+ZMqUGyatUqDh06xOzZs7HZbMTFxZGamsrevXvZv38/11xzDSUlJbS3t3P//fezYsUK4Pg0Bq2trVx++eWcd955bNq0iZycHF5//XXi4+NZvnw5V111Fddffz15eXnccccdvPHGG3i9Xv7yl78wefJkampquOWWWygvL2fRokW89957bNu2jYyMjDD/ZoTpaA1et1Hr7WgBTwt4XEaQ2OLBlhC8xRlB5q6Htvrj965aaK2C1urgrcrYzhYPdkdwH/FGzbmjxaj9djQb9163EahxSUZ4xiUZP+OuP75P7T9e1rgUyJkHk66A3AJwZBhB6+8watG+jr5ru0oZtXiLHSw2497vgbqDRi25Zi8c/hhayiFrBnztBzDhMhg939j+ZDwu42erCo0vGVsCTL4SRs3pGcKJF8H4i0L2TxYKwzbQH31jD4XlzSHd59RRSfz0m9NO+v4vf/lLdu/ezfbt2/noo4+48sor2b17d1e3vzVr1pCWlkZbWxvnnnsu1113Henp6T32ceDAAV588UVWr17NDTfcwKuvvsptt912wrEyMjL48ssv+c1vfsMTTzzBs88+y6OPPsrFF1/MQw89xNtvv83vf//7kJ6/MKH2JqjaA5W7jdpseyMkjzH+m5861vhvfqwTavYZNcaqPcatdj8EvGd37PhUSBwJjkwjcO0JRqh724zQ87ZBwG0c35kdDPBkI+g9LiPgO0O+vckI6pHTwZll3BJHGs0a6ePPuLbap3EX9nzu9546wHuzO4zzzZkXujINkWEb6MPB/Pnze/Th/vWvf81rr70GQElJCQcOHDgh0PPz85k9ezYA8+bNo7i4uM99X3vttV3b/PWvfwXg008/7dr/0qVLSU1NDeXpiMEWCEBNERz7DEo+h8pdRg021mnc7IlGWAT84HX1DEbtN9p/Uca9UtBSCY3d5mFKSDduBz8wasF9ScqFkdNgwiXGtrGJYO88fgL4fcbPetuOl8EaF9x3GsSnGfcJ6UY7diQ4nTA3uWEb6KeqSQ8Vh8PR9fijjz7i/fffZ/PmzSQkJLBkyZI++3jHxh7/I7BYLLS1tfW5787tLBYLPt8pLp6I0HPXG3/ksc5Tb+dxGc0D7nqjzddVa9y3Nx1vCuhsf3XVQtlW4z0AxwjImWs87mg12os9rcZjiy3Y3BFsvohLAmUBtNGGrIP3OXNh7u1GT4qs6UYtWCnjfVet0d7bWGwcM2OScSEvXioB0WzYBno4OJ1OWlpa+nyvqamJ1NRUEhIS2Lt3L5999lnIj7948WLWrVvHj3/8Y959910aGhpCfoyI0Nk+3PuCvlJGbTPG0vP1lio4+ikUB2+1+43XbQ5wjoTELOM+4IPWmuPtvF5X38dXMcHeF509MWKNtuKp18CYRTBmgXFxbLDGASgFiZnGLdd8zQJi8Eigd5Oens7ixYuZPn068fHxjBw5suu9pUuX8rvf/Y4pU6YwadIkFi5cGPLj//SnP+Xmm2/m+eefZ9GiRWRlZeF09lOLjAbtzVD+pdETonSbURNurTr59p09CWwOI/yaSozX7U4YsxBm3WTUiFurjGaN1iqo2GnUnBNHGG2niSONx45Mo+23s7nDkWE0ncigLTEMSbfFYaSjowOLxYLVamXz5s3cc889bN++/az2Oex+pwE/1B4wunq5qnv2juhoNnpMeN1Gc4en1bh319PVjSx9AuQUGF3CYnrVR3TAaALpbJf2uo3mkKyZkLcYsmaBReowwtyk26JJHDt2jBtuuIFAIIDdbmf16tXhLtLAdXWTazHaiT0txx+3Nxq9NMq/MoK8R1OGMporYp3BXhLBi4eJI49fRHRmGe3JOfOkjVgMqeZ2Lz6/Js1hP+t9aa2pbungSK2LsekJZCfHh6CEPUmgDyMTJ07kq6++Cncx+qa1MXCiYqfRe6Nyp9HftyMY3J7WUw8KscYZNeU5txn9eUfNhqQcI7Rjht2UQiJKuT0+thY3sOlQHZsP1bKrrImAhpFJsUwblczU7CSmjkpiTFoC/oDGF9D4/AH8AY3HH8Dt8ePq8OH2+HF7/DS1eTlW7+JIrZujdS7cHqP//WNXT+P2RXkhL78EujiRux6qi4zBFTV7jcfVheCuC26gjKaPzEnGaDu70+ge11m77n4fm2jUwFPGSnOHOCWvP8CRWhe1LR10+AJ0+PzGvTeAN2BUFrq3ENutMWQnx5GdHM+olDgS7D0/X1pr2r0BWjt8uD0+XB1+2rzGvdvjo7bVQ3VLBzUt7dS0dFDV3MHeyma8fo3NopgzOpUfXDwRZ5yVwvJm9pQ38/H+GvyBgTdTW2MUuanx5GU4WJCfxrhMB3npDqaOSgrJ7+yE4w3KXsXw5vcZ7dctlcEJhA5B3eHg/SGja14ne6IxfHrSFcZ8FVkzjX7OsYlhK74wN7fHR2VTO5VN7eyvaqGwopnCimb2V7Xi8Z350P/keBvpiXbaPX5aOny4Onz0l71KQbrDTqYzjhHOWL6zOJ+vTcjg3LzUE74gANq9fg5UtVLe1IY1RmG1xGCNUVhiFDZLDI5YCw67lQS7BUeslVhrzJDOeiqBHok6JyvydRgXG9/8ITSVGkOgW6rAVUPXRcZOzmxIGw+Tr4D0iTBiihHkfU1yJESQ1pqa1g72VrSwr7KFvZUtHK1zoYEYBTHKCDutoc7VQUVTOy3tPcddpDnsTM1OYvnX8piS7SQ7OZ5YawyxVguxthjibBasMer41FzBB+2eABVNbVQ0tVPe1EZFYzv1bg8JNiNME2OtJMZZccRacdgtJNgtxNuNx/F2CxmJsaQ77FgtA2/yi7NZmJGbzIzc5JD8/kJNAj2SeNuhpcK4CNmpvRF2vwopo8E5ymi/7ux3nZhlDB9PG2dcfBRRq83jp7a1g3qXh3qXB7s1htGpCWSnxGHrFnjtXj87S5vYUlzPtqMN7ChppM7l6Xp/hDOWcZkOrDEx+AOagNZ4/QG0hrx0BwvHpTMyKY6spDiykuOYMCKREc7YM67FjklPOOtzjyQS6GchMTGR1tZWysvLWblyJa+88soJ2yxZsoQnnniCgoI+exkB8NRTT7FixQoSEowP50Cm4+3B54HWSqONW8UEe4g4jMmKGu2w6mj/+xARocPnZ3dZE3vKm2lye2n1GE0Prg7jYl2b10+bx2/cBx83ur20ef197i9GQXZyPLmp8Xj9AXaVNeH1G/+7G5/p4OLJI5iSncTkbCeTs5JC0htEnDkJ9BAYNWpUn2E+UE899RS33XZbV6Bv2LCh/x8K+I2+1u2NxjBwMAbBJI7sOXeFkh4kw5k/oDlQ3YLPr8l0xpLmsPeoEYNxsbDe5aG2tYPW9hOniWhs8/LlsQa2FTews6ypRzu03RpDYqy1R9tunM1CcryNOLuFOKuFlASj7TnDYRw/LdFOhzdASYObkvrgraGNGKX4zuJ8CvLSmDc2VcJ7GJJA72bVqlWMHj2ae++9F4BHHnkEq9XKxo0baWhowOv18vOf/5yrr766x891n0e9ra2NO++8kx07djB58uQec7ncc889bNmyhba2Nq6//noeffRRfv3rX1NeXs5FF11ERkYGGzdu7JqONyMjgyeffJI1a9aADnDXt2/igRXfpvjQQS6/ZQXnzZ/Dpq07yBmVw+vr1xPvTBnKX5c4A+1eP9tLGtlaXM+W4ga+PNpAS0fPkE5JsJGRaMz1U9vaQaO7/1kTbRbF9Jxk7lg0lnlj05gzJqXPL4fTsYj0/jcSw8qAAl0ptRT4FWABntVa/7LX+2OAPwIpwW1Waa0HUM08hb+tMvo7h1LWDLj8lyd9+8Ybb+SBBx7oCvR169bxzjvvsHLlSpKSkqitrWXhwoUsW7bspG1+v/3tb0lISKCoqIidO3cyd+7crvd+8YtfkJaWht/v55JLLmHnzp2sXLmSJ598ko0bN3ab91xDaw3bdm3mD6t/x+dvPIfWmgVX3c6FBVNJzRjBgSMlvPjnP7G6YCE33Hwrr77+Zp/T9IrQam73cqTGRXK8jdQEO844KzExfX8WvP4A+ypb2FnaxK6yRnaUNLG/qgVfsOvFOSMT+ebsURSMNXpU1LZ2UNvaQV2rURvXGhaOSyMjMTZ4s+OMs/Vet4c4u4Wp2UnE2SwnFkJElX4DXSllAZ4GLgNKgS1KqfVa68Jum/0EWKe1/q1SaiqwAcgbhPIOqjlz5lBdXU15eTk1NTWkpqaSlZXFgw8+yCeffEJMTAxlZWVUVVWRlZXV5z4++eQTVq5cCcDMmTOZOXNm13vr1q3jmWeewefzUVFRQWFhYY/3CfiNeUX8Xmit4NNP/8E/XfUNHCPHgS2Ba6+/ib8XlrNsWYExTe+C84FTT9Mrzl6718+He6tZv72cD/dV92jSsMQokuNtOOOs+PzG4BKvP4DHF6DDF+jqs5wcb2NmbjIrJo1j3thU5o1NJSVBmixEaA2khj4fOKi1PgyglHoJuBroHuga6OwpnwyUn3XJTlGTHkzf+ta3eOWVV6isrOTGG2/khRdeoKamhm3btmGz2cjLy+tz2tz+HDlyhCeeeIItW7aQmprK8uXLe+6nrRGqK41FCVSMMR1q0i7w1hlD36HHiMqBTtMr+tfuNS4k1rZ68AYDuTOUt5c08c6eSlo7fGQkxnLrgjEsyE+ntcNHo9tDg9tDo9tLa4cPa0wMdqvCbonBZjG6252T5WRWbjJj0hKGtD+yiE4DCfQcoKTb81JgQa9tHgHeVUr9AHAAfa6ErJRaAawAGDNmzOmWdUjceOON3H333dTW1vLxxx+zbt06RowYgc1mY+PGjRw9euoeIxdccAFr167l4osvZvfu3ezcuROA5uZmHA4HycnJVFVV8be//Y0lF14IHhfOhFhayvaSMW6CsShtjBWssZx//vksX76cVatWobXmtdde4/nnnx+C34K5eXwBPj9Sx/uFVXx2uJ6UBBv5GQ7yMoxReqPT4impd7O1uIFtxxrY3a3nRm/OOCtXzMhi2awcFo1Px3KS5hUhhoNQXRS9GXhOa/2fSqlFwPNKqela95zcQ2v9DPAMGLMthujYITVt2jRaWlrIyckhOzubW2+9lW9+85vMmDGDgoICJk+efMqfv+eee7jzzjuZMmUKU6ZMYd48Y77qWbNmMWfOHCZPmsTonCwWnzvbGOxTu58Vt/wTS29/gFE5o9m4cWPXvubOncvy5cuZP38+AHfddRdz5syR5pUgrTWtHT4aXF7qXB0cq3fzflE1H+2tpqXDR5wthvn56bg6fLxXWNWjvzQYPUBm5Sbz3fPGMXdMCrmpCditxoi/zltyvA27VXoKCXPod/rcYEA/orX+RvD5QwBa6//bbZs9wFKtdUnw+WFgoda6+mT7jYrpc3UAPO7gNLDBqWA7v+MsscHlwRzGOoyDNM9JJPxOtdZUNLWzs7SRHaVN7Cxt5GB1Kw0uLx5/z6Hi6Q47l0wZwWVTszhvQgbx9uMXCpvavBytc3Gs3s2olHimj0qWsBamc7bT524BJiql8oEy4Cbgll7bHAMuAZ5TSk0B4oCaMy+yyXnbjdq3p5WuIfbWOGO9xs4Qt8gFsVOpbGrnk/01fLy/hi+K66lp6QCMyY4mZTk5b0ImGU476Q47aY5Y0hw2RjjjmJKddNJmEePCZAozc1OG8EyEGDr9BrrW2qeUug94B6NL4hqt9R6l1GPAVq31euBHwGql1IMYCbZch2vljHBrazQW9lUxx1e3sSfKTIOn0O71U9rgprjWzZbiej7eX8PeSmMpwJFJsZw/IYNZo1OYkZss3fOEOIUBpUywT/mGXq893O1xIbA4FAXSWpuzN4DWweXMKo0FgFPzwRreWvhw+k71BzSlDW4O1bRyuMbFoZpWioNzRFc0t3dNi2qzKM7NS+Ohyydz4aRMJo10mvPzIEQYDKtqY1xcHHV1daSnp5vrjzjgg4ajxhJq8WmQPDrsizZoramrqyMuLm7Ij93g8hhTopYb06IWVTRzuMbVo707NdjzZMG4dMamJwRvDiaNdOKIHVYfSyFMY1j95eTm5lJaWkpNjUma3/1eYw3LjlYj1ONTINYNlfvCXTLA+ILMzc0N+X611nx+pJ6P9tXQ6PbQ1ObtutW2GgsFdMpKimNKtpMLz8lkfGYi4zIdjMtMlHlAhBgEwyrQbTYb+fn54S7GybU3w56/wpFPoPjT4yvPp+bDNb+BsfPDW75B1ubx8/r2Mp7bVMzeyhZsFkVKgp3keBsp8bZgeCdxzshEpmYnMyXbSXpibP87FkKExLAK9GHNXQ9/WmbML5OYBfkXQN75kH++EehmaiI6Df6ApqiimTd2lPPSlhKa2rxMznLy+HUzWDYrp0e3QCFEeEmgD4Srzgjz2gNw80twztKIDfB2r59dZU18caTeWMSg2JgN0BKj+Ma0kdyxKI/5+WnmusYhRJSQQO+Pqxb+uMxYb/OWl2D8xeEuUcgEAprDtS52lDSyo7SR7SWNFFU0dw2DnzjCmA1wfl4ai8YbK80IIYYvCfRTaa02wryhGG55GcYtCXeJQqK6uZ21Xxxj7efHqA4O2HHYLczMTeGu88cxZ3QKBXlpcuFSCJORQD+ZxmPw5+uN+1vXGW3mJqa15stjDfxx01E27KrAF9AsmZTJP0/PZvaYFMZnJsrEU0KYnAQ6GIOCGo7A0c1wbJNxX3/IGCB02yuQd164S3jGGt0e1u8o5+UtJewpb8YZa+X2RXl8e9FY8jNkYWghIokEeiAAL1wHhz40nselwJhFMO8OmHQlZEwIa/HOhD+g+fRgLX/ZWsK7e6rw+ANMyU7i59dM55/m5MjAHSEilPxlb1tjhPn5/wwzrjcWlgjzKM8zUd7YxmeH69h8qI6/H6ilsrmdlAQbtywYw/XzcpmekxzuIgohBll0B3pLJbz/qHGx8+KfmK4r4lfHGnh5SwmbD9dxtM4NGAsML8hP4//MmsqlU0cQa5V+4kJEi+gO9LdXga8DrnzSVGH+1bEGfvXBAT7aV4Mz1srC8encviiPRePSmZzlPOmixUKIyBa9gb7/XdjzGlz0E0gfH+7SDEj3IE9NsPGvSydx+6I8EqVNXAhBtAa6xwVv/choL1+8Mtyl6deRWhf/vqGI9wqrSJEgF0KcRHQmwsePQ9MxWL4BrMN38qgmt5dffXCAP20uJtYaw48uO4c7z8uXIBdC9Cn6kqFqD2x+GubcBnkhWZMj5Lz+AC98dpSnPjhAU5uXGwtG88Ovn8MIpwy9F0KcXHQFelsDvH4vxCXDZT8Ld2lOEAho3thZzlPvH+BIrYuvjU/nJ1dOZeqopHAXTQhhAtET6OVfwbrbobkCrl8DCWnhLlEXrTXvFVbx5Hv72VvZwuQsJ8/eXsAlU0bIrIZCiAGL/EDXGrY9B3/7MTgy4TtvQ25BuEvVZdOhWh5/ex87ShrJz3Dw65vncNWMbOl6KIQ4bZEd6B43vPVD2PEijL8Erl0NjvRwlwqApjYvv3irkHVbS8lJiec/rpvJtXNzsFrMN0pVCDE8RG6gtzXCc1caF0GXPAQX/AvEDI9Rkx8UVfFvr+2ittXDPUvGc/8lE4mzDY+yCSHMK3ID/f1HoLoQblkH53w93KUBoMHl4bE3C3ntqzImZzlZfXsBM3NTwl0sIUSEiMxAP7oJtv0BFt03bML8w71V/Osru2h0e3jg0ol8f8kE7FZpXhFChE7kBbqvA964H1LGwEX/Fu7S4Pb4+PlbRaz9/BiTs5z86TvzpRuiEGJQRF6g//0/oXY/3PYq2MO7gMNXxxp48OXtHK13870Lx/HDy86R2Q+FEIMmsgK9ei/8/UmYcQNMuDRsxfD6A/z3hwf5740HyUqK48W7F7Jw3PDoXSOEiFwDCnSl1FLgV4AFeFZr/cte7/8XcFHwaQIwQmudEsJy9i8QgDdWQmwifOPfh/TQ3R2sbuHBl3ewq6yJa+fm8MiyaSTF2cJWHiFE9Og30JVSFuBp4DKgFNiilFqvtS7s3EZr/WC37X8AzBmEsp7atjVQ8jlc81tIzBzywwcCmj9sKubxt/fisFv47a1zuXxG9pCXQwgRvQZSQ58PHNRaHwZQSr0EXA0UnmT7m4GfhqZ4A9RccXzloVk3D+mhAUob3PzzX3bw2eF6Lp0ygn+/doZMpCWEGHIDCfQcoKTb81JgQV8bKqXGAvnAhyd5fwWwAmDMmDGnVdBT2vgL8LWHZeWhvx+o4Z4/f4nWmv+4bibfKsiV+VeEEGER6ouiNwGvaK39fb2ptX4GeAagoKBAh+SI1UWw/QVYcM+Qrzy0+VAdd/9pK3npDlbfXsDotIQhPb4QQnQ3kEAvA0Z3e54bfK0vNwH3nm2hTsv7j4DdCRf885AedtvRer77xy2MTk3ghbsWkJ44fBfKEEJEh4EMVdwCTFRK5Sul7Bihvb73RkqpyUAqsDm0RTyF4n/A/rfhvAeGdDrcHSWNLF+zhZFJcRLmQohho99A11r7gPuAd4AiYJ3Weo9S6jGl1LJum94EvKS1Dk1TSv8Fg/ceBucoWHjPkBwSYE95E7ev+YIUh421dy9gRJJc/BRCDA8DakPXWm8ANvR67eFezx8JXbEGoPB1KNsKy/4bbPFDcsiD1S18+/df4LBbWHvXQrKTh+a4QggxEOacHcrvhQ8eg8wpMPuWITmk2+Pje89vI0YpXrh7oVwAFUIMO+Yc+r/tOag/BDe/PGRznP/09T0crnXx5+8uID8jvHPECCFEX8xXQ+9ogY8fh7GL4ZxvDMkh/+erMv6yrZT7LprA4gkZQ3JMIYQ4XearoW/+Dbhq4OaXhmQQUXGti//92i7OzUvl/ksmDvrxhBDiTJkv0AvuBGfWkCz03OHzc9+LX2K1xPCrm+bIep9CiGHNfIGeOALm3TEkh3r8b/vYXdbM6tsLGJUiPVqEEMObVDlP4r3CKtb84wjLv5bHZVNHhrs4QgjRLwn0PhRVNPPgy9uZnpPEQ1dMDndxhBBiQCTQe6lqbuc7z20hMdbKs7efK0vGCSFMQwK9G1eHj+88t4XmNi9rlp9LVrIM6xdCmIf5LooOEn9Ac/9LX1FU0czv7ziXqaOSwl0kIYQ4LRLoQT97s5D3i6r52dXTuGjyiHAXRwghTps0uQB/3FTMc5uK+e55+Xx7UV64iyOEEGck6gO90e3h8bf3smRSJv92xZRwF0cIIc5Y1Af6nz87itvjZ9Xlk7HEyFqgQgjziupAb/f6eW5TMUsmZTI5Sy6CCiHMLaoD/dUvS6lt9fC9C4Z2cWkhhBgMURvo/oBm9SeHmZWbzMJxQ7ceqRBCDJaoDfR391RSXOfmexeORw3BNLxCCDHYojLQtdb87pPDjE1P4BvTssJdHCGECImoDPQvjtSzo6SRu84fJz1bhBARIyoD/f99cph0h51vzcsNd1GEECJkoi7Q91W28OHeau74Wh5xNplJUQgROaIu0J/55DDxNgvfXjg23EURQoiQiqpAb3J7Wb+jjBsKckl12MNdHCGECKmoCvR3Civx+jXXSdu5ECICRVWgv7WzgtzUeGbkJIe7KEIIEXIDCnSl1FKl1D6l1EGl1KqTbHODUqpQKbVHKbU2tMU8e41uD/84WMuVM7NlIJEQIiL1u8CFUsoCPA1cBpQCW5RS67XWhd22mQg8BCzWWjcopYbdChHv7qnCF9BcOSM73EURQohBMZAa+nzgoNb6sNbaA7wEXN1rm7uBp7XWDQBa6+rQFvPsvbmrgtFp0twihIhcAwn0HKCk2/PS4GvdnQOco5T6h1LqM6XU0r52pJRaoZTaqpTaWlNTc2YlPgMNLg+bDtZy5YxR0twihIhYobooagUmAkuAm4HVSqmU3htprZ/RWhdorQsyMzNDdOj+vVtYKc0tQoiIN5BALwNGd3ueG3ytu1Jgvdbaq7U+AuzHCPhh4a1dlYxJS2B6jixiIYSIXAMJ9C3ARKVUvlLKDtwErO+1zf9g1M5RSmVgNMEcDl0xz1yDS3q3CCGiQ7+BrrX2AfcB7wBFwDqt9R6l1GNKqWXBzd4B6pRShcBG4F+01nWDVejT8c6eSvzS3CKEiAL9dlsE0FpvADb0eu3hbo818MPgbVh5a1cFY9MTmDZKmluEEJEtokeK1rs8bDpUx5UzpLlFCBH5IjrQ3w02t1whzS1CiCgQ0YH+1q4K8qS5RQgRJSI20BvdRnPLFdLcIoSIEhEb6NtLGvEHNOdPHLoBTEIIEU4RG+h7ypsBmCaDiYQQUSJiA313WRN56QkkxdnCXRQhhBgSkRvo5U1Mk5kVhRBRJCIDvcntpaS+jemjJNCFENEjIgN9T3kTgEzGJYSIKhEZ6LvKgoEuNXQhRBSJyEDfXd5MTko8qQ57uIsihBBDJiIDfU9ZkzS3CCGiTsQFeku7l8O1LmluEUJEnYgL9KKKFgCmS5dFIUSUibhA3x28ICojRIUQ0SbyAr28iRHOWEY448JdFCGEGFKRF+hlTdLcIoSIShEV6G0ePwerWyXQhRBRKaICvaiymYCG6bKghRAiCkVUoO/pHCEqNXQhRBSKqEDfXdZMmsNOdrJcEBVCRJ/ICvTyJqaNSpIl54QQUSliAr3D52d/VYs0twgholbEBPr+yla8fi1D/oUQUStiAn13cA70GVJDF0JEqQEFulJqqVJqn1LqoFJqVR/vL1dK1Siltgdvd4W+qKe2u6wJZ5yV0WnxQ31oIYQYFqz9baCUsgBPA5cBpcAWpdR6rXVhr01f1lrfNwhlHJDd5c1MH5UsF0SFEFFrIDX0+cBBrfVhrbUHeAm4enCLdXq8/gBFFc0yB7oQIqoNJNBzgJJuz0uDr/V2nVJqp1LqFaXU6JCUboAOVrfi8QWkh4sQIqqF6qLoG0Ce1nom8B7wx742UkqtUEptVUptrampCdGhobjWBcCEEYkh26cQQpjNQAK9DOhe484NvtZFa12nte4IPn0WmNfXjrTWz2itC7TWBZmZmWdS3j7Vuz0ApDtiQ7ZPIYQwm4EE+hZgolIqXyllB24C1nffQCmV3e3pMqAodEXsX6PbC0BKgm0oDyuEEMNKv71ctNY+pdR9wDuABVijtd6jlHoM2Kq1Xg+sVEotA3xAPbB8EMt8ggaXhwS7hTibZSgPK4QQw0q/gQ6gtd4AbOj12sPdHj8EPBTaog1cvdtDaoI9XIcXQohhISJGija6vdLcIoSIehER6A1SQxdCiMgIdKmhCyFEhAR6g9tDmkNq6EKI6Gb6QPcHNE1tXlKkyUUIEeVMH+hNbV60hlRpchFCRDnTB3pDcJSoXBQVQkQ70wd6YzDQ5aKoECLamT7QG1zGsH+5KCqEiHamD/R6aXIRQgggAgJdmlyEEMJg+kBvcHuxxigSYwc0LY0QQkQs0wd6o9tDqsMua4kKIaKe6QO93uWRPuhCCEEEBHqDW0aJCiEERECgN7qlhi6EEBABgd7g9kqXRSGEwOSBrrXuuigqhBDRztSB3trhw+vX0uQihBCYPNAb3cawf7koKoQQJg90mWlRCCGOM3mgd07MJU0uQghh7kB3dc7jIjV0IYQwd6BLk4sQQnQxeaB7UQqS46XJRQghTB3ojW4PSXE2LDEyMZcQQpg60BvcXlmpSAghggYU6EqppUqpfUqpg0qpVafY7jqllFZKFYSuiCfX4PLIwhZCCBHUb6ArpSzA08DlwFTgZqXU1D62cwL3A5+HupAn0+D2yAVRIYQIGkgNfT5wUGt9WGvtAV4Cru5ju58BjwPtISzfKTW6vVJDF0KIoIEEeg5Q0u15afC1LkqpucBorfVbp9qRUmqFUmqrUmprTU3NaRe2N6mhCyHEcWd9UVQpFQM8Cfyov2211s9orQu01gWZmZlnddx2rx+3xy8XRYUQImgggV4GjO72PDf4WicnMB34SClVDCwE1g/2hdHjE3NJk4sQQsDAAn0LMFEpla+UsgM3Aes739RaN2mtM7TWeVrrPOAzYJnWeuuglDhIRokKIURP/Qa61toH3Ae8AxQB67TWe5RSjymllg12AU+mM9Clhi6EEAbrQDbSWm8ANvR67eGTbLvk7IvVv8aumRalhi6EEGDikaL1LmlyEUKI7kwb6I3S5CKEED2YNtAb3F4S7BZirZZwF0UIIYYFEwe6DCoSQojuzBvoLg+psvScEEJ0MW+gu71SQxdCiG5MG+iNbo+sJSqEEN2YNtCNGro0uQghRCdTBrrPH6C5XZpchBCiO1MGelObF62RGroQQnRjykBvCA77T5Vh/0II0cWUgX58lKgEuhBCdDJloHfV0KXJRQghupg00GViLiGE6M2cgd4506K0oQshRBdzBrrbi82icNhlYi4hhOhkykDvHCWqlAp3UYQQYtgwZaA3uD2kSfu5EEL0YM5Ad3llYQshhOjFnIEuc6ELIcQJTBroXpkLXQghejFdoGutZepcIYTog+kCvbXDhy+g5aKoEEL0YrpAb3AZw/7loqgQQvRkvkCXYf9CCNEn8wa6XBQVQogeTBfoje7OJhepoQshRHcDCnSl1FKl1D6l1EGl1Ko+3v9fSqldSqntSqlPlVJTQ19UQ31wYi65KCqEED31G+hKKQvwNHA5MBW4uY/AXqu1nqG1ng38B/BkqAvaKTc1nq9PHUlSvDS5CCFEd9YBbDMfOKi1PgyglHoJuBoo7NxAa93cbXsHoENZyO6+Pi2Lr0/LGqzdCyGEaQ0k0HOAkm7PS4EFvTdSSt0L/BCwAxf3tSOl1ApgBcCYMWNOt6xCCCFOIWQXRbXWT2utxwM/Bn5ykm2e0VoXaK0LMjMzQ3VoIYQQDCzQy4DR3Z7nBl87mZeAa86iTEIIIc7AQAJ9CzBRKZWvlLIDNwHru2+glJrY7emVwIHQFVEIIcRA9NuGrrX2KaXuA94BLMAarfUepdRjwFat9XrgPqXUpYAXaADuGMxCCyGEONFALoqitd4AbOj12sPdHt8f4nIJIYQ4TaYbKSqEEKJvEuhCCBEhlNaDNgbo1AdWqgY4eoY/ngHUhrA44RZJ5xNJ5wJyPsNZJJ0LDPx8xmqt++z3HbZAPxtKqa1a64JwlyNUIul8IulcQM5nOIukc4HQnI80uQghRISQQBdCiAhh1kB/JtwFCLFIOp9IOheQ8xnOIulcIATnY8o2dCGEECcyaw1dCCFELxLoQggRIUwX6P0thzfcKaXWKKWqlVK7u72WppR6Tyl1IHifGs4yDpRSarRSaqNSqlAptUcpdX/wdbOeT5xS6gul1I7g+TwafD1fKfV58DP3cnCSOlNQSlmUUl8ppd4MPjfzuRR3W+pya/A1s37WUpRSryil9iqlipRSi0JxLqYK9AEuhzfcPQcs7fXaKuADrfVE4IPgczPwAT/SWk8FFgL3Bv89zHo+HcDFWutZwGxgqVJqIfA48F9a6wkYk899N3xFPG33A0Xdnpv5XAAu0lrP7tZf26yftV8Bb2utJwOzMP6Nzv5ctNamuQGLgHe6PX8IeCjc5TqD88gDdnd7vg/IDj7OBvaFu4xneF6vA5dFwvkACcCXGKtz1QLW4Os9PoPD+YaxdsEHGCuIvQkos55LsLzFQEav10z3WQOSgSMEO6WE8lxMVUOn7+XwcsJUllAaqbWuCD6uBEaGszBnQimVB8wBPsfE5xNsotgOVAPvAYeARq21L7iJmT5zTwH/CgSCz9Mx77mAsVbxu0qpbcHlLMGcn7V8oAb4Q7A57FmllIMQnIvZAj3iaePr2VR9SZVSicCrwAO654LhpjsfrbVfaz0bo3Y7H5gc3hKdGaXUVUC11npbuMsSQudprediNLneq5S6oPubJvqsWYG5wG+11nMAF72aV870XMwW6Ke7HJ5ZVCmlsgGC99VhLs+AKaVsGGH+gtb6r8GXTXs+nbTWjcBGjGaJFKVU59oBZvnMLQaWKaWKMZaFvBij3daM5wKA1roseF8NvIbxhWvGz1opUKq1/jz4/BWMgD/rczFboPe7HJ5Jref4Kk93YLRFD3tKKQX8HijSWj/Z7S2znk+mUiol+Dge43pAEUawXx/czBTno7V+SGudq7XOw/g7+VBrfSsmPBcApZRDKeXsfAx8HdiNCT9rWutKoEQpNSn40iVAIaE4l3BfIDiDCwpXAPsx2jb/d7jLcwblfxGowFiurxSjl0E6xsWrA8D7QFq4yznAczkP47+FO4HtwdsVJj6fmcBXwfPZDTwcfH0c8AVwEPgLEBvusp7meS0B3jTzuQTLvSN429P5t2/iz9psYGvws/Y/QGoozkWG/gshRIQwW5OLEEKIk5BAF0KICCGBLoQQEUICXQghIoQEuhBCRAgJdCGEiBAS6EIIESH+PxHYSixSzXzxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "345ccd9e90e0e8c52c007d54ee8499f3e9cbf6e28ceb6819fa98b9b77c2e4dd3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
